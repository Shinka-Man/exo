# Kimi-K2 1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«åˆ†æ•£æ¨è«– å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸš€ æ¦‚è¦
æœ¬ã‚¬ã‚¤ãƒ‰ã¯ã€Moonshot AIè£½ã®Kimi-K2ï¼ˆ1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€504GBï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ã€Apple Silicon Mac 2å°ã§åˆ†æ•£æ¨è«–ã™ã‚‹æ–¹æ³•ã‚’è©³ç´°ã«è§£èª¬ã—ã¾ã™ã€‚

### é”æˆå†…å®¹
- **ãƒ¢ãƒ‡ãƒ«**: Kimi-K2-Instruct-MLX-3.985bitï¼ˆ504GBï¼‰
- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢**: M2 Studio (192GB) + M3 Ultra (512GB)
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: exo + MLX
- **å®Ÿç¸¾**: å®Œå…¨å‹•ä½œã€ãƒˆãƒ¼ã‚¯ãƒ³ç”ŸæˆæˆåŠŸ

## ğŸ“‹ å‰ææ¡ä»¶

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶
- **Mac 1**: 192GBä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªï¼ˆM2 Ultraæ¨å¥¨ï¼‰
- **Mac 2**: 512GBä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªï¼ˆM3 Ultraæ¨å¥¨ï¼‰
- **æ¥ç¶š**: Thunderbolt 4ã‚±ãƒ¼ãƒ–ãƒ«ï¼ˆ10Gbpsä»¥ä¸Šï¼‰

### ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶
- macOS 14.0ä»¥ä¸Š
- Python 3.10ä»¥ä¸Š
- 500GBä»¥ä¸Šã®ç©ºããƒ‡ã‚£ã‚¹ã‚¯å®¹é‡

## ğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. exoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä¸¡æ–¹ã®Macã§ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/exo-explore/exo.git
cd exo

# Pythonä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python3 -m venv venv
source venv/bin/activate

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e .

# è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ï¼ˆKimi-K2ç”¨ï¼‰
pip install tiktoken blobfile
```

### 2. ãƒ¢ãƒ‡ãƒ«ã®é…ç½®

ä¸¡æ–¹ã®Macã§åŒã˜ãƒ‘ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã‚’é…ç½®ï¼š

```bash
# ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p ~/models/inferencerlabs

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆHuggingFaceã‹ã‚‰ï¼‰
# ã¾ãŸã¯æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
cp -r /path/to/Kimi-K2-Instruct-MLX-3.985bit ~/models/inferencerlabs/
```

### 3. å¿…é ˆãƒ‘ãƒƒãƒã®é©ç”¨

#### 3.1 sharded_utils.py ã®ä¿®æ­£

`exo/exo/inference/mlx/sharded_utils.py` ã®460è¡Œç›®ä»˜è¿‘ã«ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼š

```python
# 460è¡Œç›®ä»˜è¿‘ã€quantizeé–¢æ•°å†…ã«è¿½åŠ 
# Precompute per-module params from weights
per_module_params = {}
try:
    def _canon(path):
        # Canonicalize path
        if path.startswith("model."):
            return path[6:]
        if path.startswith("language_model.model."):
            return path[21:]
        return path

    for k in keyset:
        if not k.endswith('.scales'):
            continue
        base = k[:-7]  # Remove .scales
        weight_key = f"{base}.weight"
        if weight_key not in weights:
            continue
        
        # Get shapes
        G = int(weights[k].shape[1])
        P = int(weights[weight_key].shape[1])
        
        # Try different bit widths
        for b in [6, 4, 3, 2, 8]:
            for gs in [64, 128, 32, 16, 8]:
                if P * 32 == G * gs * b:
                    canon_path = _canon(base)
                    per_module_params[canon_path] = {
                        "group_size": gs,
                        "bits": b
                    }
                    if DEBUG >= 6:
                        print(f"[quantize] Pre-computed for {canon_path}: bits={b}, group_size={gs}")
                    break
except Exception as e:
    if DEBUG >= 2:
        print(f"[quantize] per-module params precompute failed: {e}")
```

ã•ã‚‰ã«495è¡Œç›®ä»˜è¿‘ã®`derive_params_from_weights`é–¢æ•°ã‚’ä¿®æ­£ï¼š

```python
def derive_params_from_weights(path: str):
    # ... æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ ...
    
    # bit_candidates ã®è¡Œã‚’ä»¥ä¸‹ã«å¤‰æ›´
    bit_candidates = [6, 4, 3, 2, 8]  # 6ã‚’æœ€åˆã«è¿½åŠ 
    
    # ... æ®‹ã‚Šã®ã‚³ãƒ¼ãƒ‰ ...
```

#### 3.2 main.py ã®ä¿®æ­£

`exo/exo/main.py` ã®240è¡Œç›®ä»˜è¿‘ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªï¼š

```python
# Preemptively load the shard for this node when a prompt starts anywhere
def preemptively_load_shard(request_id: str, opaque_status: str):
    try:
        status = json.loads(opaque_status)
        if status.get("type") != "node_status" or status.get("status") != "start_process_prompt":
            return
        base = status.get("base_shard") or status.get("shard")
        if not base:
            return
        base_shard = Shard.from_dict(base)
        current_shard = node.get_current_shard(base_shard)
        if DEBUG >= 2:
            print(f"Preemptively starting ensure_shard for {current_shard}")
        asyncio.create_task(node.inference_engine.ensure_shard(current_shard))
    except Exception as e:
        if DEBUG >= 2:
            print(f"Failed to preemptively start ensure_shard: {e}")
            traceback.print_exc()

node.on_opaque_status.register("preemptively_load_shard").on_next(preemptively_load_shard)
```

ã‚‚ã—å­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

### 4. ãƒ¢ãƒ‡ãƒ«è¨­å®šã®è¿½åŠ 

`exo/exo/models.py` ã«ä»¥ä¸‹ã‚’è¿½åŠ ï¼š

```python
"kimi-k2-4bit": {
    "MLXDynamicShardInferenceEngine": {
        "repo": "inferencerlabs/Kimi-K2-Instruct-MLX-3.985bit",
        "local_path": Path.home() / "models/inferencerlabs/Kimi-K2-Instruct-MLX-3.985bit"
    },
    "layers": 61
}
```

### 5. ãƒ¢ãƒ‡ãƒ«config.jsonã®ä¿®æ­£

`~/models/inferencerlabs/Kimi-K2-Instruct-MLX-3.985bit/config.json` ã‚’ç·¨é›†ï¼š

```json
{
  "model_type": "deepseek_v3",  // kimi_k2 ã‹ã‚‰å¤‰æ›´
  // ... ä»–ã®è¨­å®šã¯ãã®ã¾ã¾
}
```

## ğŸš€ èµ·å‹•æ‰‹é †

### 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š

#### Thunderbolt Bridge ã®è¨­å®š
1. ä¸¡æ–¹ã®Macã‚’Thunderbolt 4ã‚±ãƒ¼ãƒ–ãƒ«ã§æ¥ç¶š
2. ã‚·ã‚¹ãƒ†ãƒ è¨­å®š â†’ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ â†’ Thunderbolt Bridge
3. IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ‰‹å‹•è¨­å®šï¼š
   - M2å´: 10.0.2.1/24
   - M3å´: 10.0.2.2/24

### 2. Discoveryè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

ä¸¡æ–¹ã®Macã§ `/Users/{username}/exo/discovery.json` ã‚’ä½œæˆï¼š

```json
{
  "peers": {
    "m2-studio-main": {
      "address": "10.0.2.1",
      "port": 50051,
      "device_capabilities": { 
        "model": "Mac Studio",
        "chip": "Apple M2 Ultra",
        "memory": 196608,
        "flops": { "fp32": 26.98, "fp16": 53.96, "int8": 107.92 }
      }
    },
    "m3-studio-worker": {
      "address": "10.0.2.2",
      "port": 50052,
      "device_capabilities": {
        "model": "Mac Studio",
        "chip": "Apple M3 Ultra",
        "memory": 524288,
        "flops": { "fp32": 54.26, "fp16": 108.52, "int8": 217.04 }
      }
    }
  }
}
```

### 3. ãƒãƒ¼ãƒ‰ã®èµ·å‹•

#### M3å´ï¼ˆWorkerï¼‰ã‚’å…ˆã«èµ·å‹•

```bash
cd ~/exo/exo
source venv/bin/activate

DEBUG=3 HF_HUB_DISABLE_TELEMETRY=1 python -m exo.main \
  --disable-tui \
  --inference-engine mlx \
  --default-model kimi-k2-4bit \
  --node-id m3-studio-worker \
  --node-host 10.0.2.2 \
  --node-port 50052 \
  --discovery-module manual \
  --discovery-config-path ~/exo/discovery.json \
  --wait-for-peers 0
```

#### M2å´ï¼ˆMainï¼‰ã‚’èµ·å‹•

```bash
cd ~/exo/exo
source venv/bin/activate

DEBUG=3 HF_HUB_DISABLE_TELEMETRY=1 python -m exo.main \
  --disable-tui \
  --inference-engine mlx \
  --default-model kimi-k2-4bit \
  --node-id m2-studio-main \
  --node-host 10.0.2.1 \
  --node-port 50051 \
  --discovery-module manual \
  --discovery-config-path ~/exo/discovery.json \
  --wait-for-peers 1
```

## ğŸ“ æ¨è«–ã®å®Ÿè¡Œ

### APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±

```bash
curl -X POST http://10.0.2.1:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "kimi-k2-4bit",
    "messages": [
      {"role": "user", "content": "Hello, tell me about artificial intelligence"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³çµŒç”±

```bash
DEBUG=3 python -m exo.main run kimi-k2-4bit \
  --prompt "Hello, how are you?" \
  --disable-tui \
  --node-id m2-studio-main \
  --node-host 10.0.2.1 \
  --node-port 50051 \
  --discovery-module manual \
  --discovery-config-path ~/exo/discovery.json \
  --wait-for-peers 1
```

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: "ValueError: Expected shape (163840, 672) but received shape (163840, 1344)"

**è§£æ±ºç­–**: sharded_utils.pyã®ãƒ‘ãƒƒãƒãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã€‚ç‰¹ã«å±¤åˆ¥é‡å­åŒ–ã®æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã€‚

### 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: "No chat template defined"

**è§£æ±ºç­–**: 
```bash
pip install tiktoken blobfile
```

### 3. æ¥ç¶šã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: ãƒãƒ¼ãƒ‰é–“ã§é€šä¿¡ã§ããªã„

**è§£æ±ºç­–**:
- Thunderbolt Bridgeã®è¨­å®šç¢ºèª
- ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã®ç„¡åŠ¹åŒ–
- `ping 10.0.2.1` / `ping 10.0.2.2` ã§ç–é€šç¢ºèª

### 4. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãŒé…ã„

**ç—‡çŠ¶**: åˆå›èµ·å‹•æ™‚ã«é•·æ™‚é–“ã‹ã‹ã‚‹

**å¯¾ç­–**:
- åˆå›ã¯æœ€å¤§30åˆ†ç¨‹åº¦ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚‹
- 2å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚Šé«˜é€ŸåŒ–
- Activity Monitorã§Python ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- **M2å´**: ç´„145-150GBï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼44-60ã‚’æ‹…å½“ï¼‰
- **M3å´**: ç´„360-370GBï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼0-43ã‚’æ‹…å½“ï¼‰

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- åˆå›ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ: ç´„50-60ç§’
- ä»¥é™ã®ãƒˆãƒ¼ã‚¯ãƒ³: ç´„5-10ç§’/ãƒˆãƒ¼ã‚¯ãƒ³
- ãƒ¡ãƒ¢ãƒªã‚¹ãƒ¯ãƒƒãƒ—ãªã—ã§å‹•ä½œ

## ğŸ¯ æˆåŠŸã®ç¢ºèª

ä»¥ä¸‹ãŒç¢ºèªã§ãã‚Œã°æˆåŠŸã§ã™ï¼š

1. ä¸¡ãƒãƒ¼ãƒ‰ã§ãƒ¡ãƒ¢ãƒªãŒæ­£ã—ãåˆ†æ•£ãƒ­ãƒ¼ãƒ‰
2. ãƒˆãƒ¼ã‚¯ãƒ³ãŒç¶™ç¶šçš„ã«ç”Ÿæˆã•ã‚Œã‚‹
3. ã‚¨ãƒ©ãƒ¼ãªãæ¨è«–ãŒå®Œäº†ã™ã‚‹

## ğŸ“š æŠ€è¡“è©³ç´°

### å±¤åˆ¥é‡å­åŒ–ã®ä»•çµ„ã¿
- **embed_tokens**: 3bité‡å­åŒ–ï¼ˆæœ€ã‚‚åœ§ç¸®ï¼‰
- **attentionå±¤ (q/k/v)**: 4bité‡å­åŒ–ï¼ˆæ¨™æº–ï¼‰
- **lm_head**: 6bité‡å­åŒ–ï¼ˆç²¾åº¦é‡è¦–ï¼‰

### ãƒ¡ãƒ¢ãƒªåˆ†å‰²æˆ¦ç•¥
- ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ãƒ™ãƒ¼ã‚¹ã§ã¯ãªããƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²
- M3ã«ã‚ˆã‚Šå¤šãã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å‰²ã‚Šå½“ã¦ï¼ˆãƒ¡ãƒ¢ãƒªå®¹é‡ã«æ¯”ä¾‹ï¼‰

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ã“ã®ã‚¬ã‚¤ãƒ‰ã®æ”¹å–„ã‚„å•é¡Œå ±å‘Šã¯ä»¥ä¸‹ã¸ï¼š
- GitHub Issues: [ã‚ãªãŸã®ãƒªãƒã‚¸ãƒˆãƒªURL]
- æ”¹å–„ææ¡ˆã¯Pull Requestã§ãŠé¡˜ã„ã—ã¾ã™

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ã‚¬ã‚¤ãƒ‰ã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯Moonshot AIã®è¦ç´„ã«å¾“ã£ã¦ãã ã•ã„ã€‚

---

*Last updated: 2025-09-01*
*Successfully tested with Kimi-K2 1T model on M2 Studio + M3 Ultra*